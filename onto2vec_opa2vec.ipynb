{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we run two ontology based methods to produce vector representations of biological entities: Onto2Vec and OPA2Vec.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onto2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onto2vec produces vectory representations based on the logical axioms of an ontology and the known associations between ontology classes and biological entities. In the case study below, we use Onto2vec to produce vector representations of proteins based on their GO annotations and the GO logical axioms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/slater/machine-learning-with-ontologies/onto2vec\n",
      "Traceback (most recent call last):\n",
      "  File \"runWord2Vec.py\", line 13, in <module>\n",
      "    ssmodel =gensim.models.Word2Vec(sentences,min_count=0, size=200, window=10, sg=1, negative=4, iter=5)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py\", line 748, in __init__\n",
      "    fast_version=FAST_VERSION)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/gensim/models/base_any2vec.py\", line 630, in __init__\n",
      "    self.build_vocab(sentences, trim_rule=trim_rule)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/gensim/models/base_any2vec.py\", line 801, in build_vocab\n",
      "    sentences, progress_per=progress_per, trim_rule=trim_rule)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py\", line 1472, in scan_vocab\n",
      "    for sentence_no, sentence in enumerate(sentences):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py\", line 1384, in __iter__\n",
      "    with utils.smart_open(self.source) as fin:\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/smart_open/smart_open_lib.py\", line 181, in smart_open\n",
      "    fobj = _shortcut_open(uri, mode, **kw)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/smart_open/smart_open_lib.py\", line 287, in _shortcut_open\n",
      "    return io.open(parsed_uri.uri_path, mode, **open_kwargs)\n",
      "IOError: [Errno 2] No such file or directory: 'AllAxioms.lst'\n"
     ]
    }
   ],
   "source": [
    "org_id ='4932' #or 9606 for human data \n",
    "%cd ../onto2vec\n",
    "!python runWord2Vec.py  -ontology ../data/tripulate/merged_pubmed_gloo.owl -associations data/train/9606.OPA_associations.txt -outfile data/9606.onto2vec_vecs -entities data/train/9606.protein_list.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## OPA2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the ontology axioms and their entity associations, OPA2Vec also uses the ontology metadata and literature to represent biological entities. The code below runs OPA2Vec on GO and protein-GO associations to produce protein vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/slater/machine-learning-with-ontologies/opa2vec\n",
      "\t\n",
      "\t\t*********** OPA2Vec Running ... ***********\n",
      "\n",
      "\n",
      "\t\t1.Ontology Processing ...\n",
      "\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "Loading of Axioms ...\n",
      "Loading ...\n",
      "    1%\n",
      "    2%\n",
      "    3%\n",
      "    4%\n",
      "    5%\n",
      "    6%\n",
      "    7%\n",
      "    8%\n",
      "    9%\n",
      "    10%\n",
      "    11%\n",
      "    12%\n",
      "    13%\n",
      "    14%\n",
      "    15%\n",
      "    16%\n",
      "    17%\n",
      "    18%\n",
      "    19%\n",
      "    20%\n",
      "    21%\n",
      "    22%\n",
      "    23%\n",
      "    24%\n",
      "    25%\n",
      "    26%\n",
      "    27%\n",
      "    28%\n",
      "    29%\n",
      "    30%\n",
      "    31%\n",
      "    32%\n",
      "    33%\n",
      "    34%\n",
      "    35%\n",
      "    37%\n",
      "    38%\n",
      "    39%\n",
      "    40%\n",
      "    41%\n",
      "    42%\n",
      "    43%\n",
      "    44%\n",
      "    45%\n",
      "    46%\n",
      "    47%\n",
      "    48%\n",
      "    49%\n",
      "    50%\n",
      "    51%\n",
      "    52%\n",
      "    53%\n",
      "    54%\n",
      "    55%\n",
      "    56%\n",
      "    57%\n",
      "    58%\n",
      "    59%\n",
      "    60%\n",
      "    61%\n",
      "    62%\n",
      "    63%\n",
      "    64%\n",
      "    65%\n",
      "    66%\n",
      "    67%\n",
      "    68%\n",
      "    69%\n",
      "    70%\n",
      "    71%\n",
      "    72%\n",
      "    73%\n",
      "    74%\n",
      "    75%\n",
      "    76%\n",
      "    77%\n",
      "    78%\n",
      "    79%\n",
      "    80%\n",
      "    81%\n",
      "    82%\n",
      "    83%\n",
      "    84%\n",
      "    85%\n",
      "    86%\n",
      "    87%\n",
      "    88%\n",
      "    89%\n",
      "    90%\n",
      "    91%\n",
      "    92%\n",
      "    93%\n",
      "    94%\n",
      "    95%\n",
      "    96%\n",
      "    97%\n",
      "    98%\n",
      "    99%\n",
      "    ... finished\n",
      "    ... finished\n",
      "Property Saturation Initialization ...\n",
      "    ... finished\n",
      "Reflexive Property Computation ...\n",
      "    ... finished\n",
      "Object Property Hierarchy and Composition Computation ...\n",
      "    1%\n",
      "    12%\n",
      "    29%\n",
      "    55%\n",
      "    91%\n",
      "    ... finished\n",
      "Context Initialization ...\n",
      "    ... finished\n",
      "Consistency Checking ...\n",
      "    11%\n",
      "    26%\n",
      "    45%\n",
      "    64%\n",
      "    100%\n",
      "    ... finished\n",
      "Caught: org.semanticweb.owlapi.reasoner.InconsistentOntologyException: Inconsistent ontology\n",
      "org.semanticweb.owlapi.reasoner.InconsistentOntologyException: Inconsistent ontology\n",
      "\tat org.semanticweb.elk.owlapi.ElkExceptionConverter.convert(ElkExceptionConverter.java:70)\n",
      "\tat org.semanticweb.elk.owlapi.ElkExceptionConverter.convert(ElkExceptionConverter.java:88)\n",
      "\tat org.semanticweb.elk.owlapi.ElkConverter.convert(ElkConverter.java:114)\n",
      "\tat org.semanticweb.elk.owlapi.ElkReasoner.precomputeInferences(ElkReasoner.java:950)\n",
      "\tat ProcessOntology.run(ProcessOntology.groovy:75)\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t2.Metadata Extraction ...\n",
      "\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t3.Propagate Associations through hierarchy ...\n",
      "\n",
      "\n",
      "   ######################################################################\n",
      "\n",
      "\n",
      "\t\t4.Corpus Creation ...\n",
      "\n",
      "\n",
      "  ######################################################################\n",
      "\n",
      "\n",
      "\t\t5.Running Word2Vec ... \n",
      "\n",
      "\t\t*********** Vector representations created ***********\n",
      "\n",
      "\n",
      "\t\t*********** OPA2Vec Complete ***********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd opa2vec\n",
    "!python runOPA2Vec.py  -ontology ../data/tripulate/merged_pubmed_gloo.owl -associations ../data/train/9606.OPA_associations.txt -outfile ../data/9606.opa2vec_vecs -entities ../data/train/9606.protein_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map proteins to corresponding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_id = '9606' #org_id = '4932'\n",
    "onto2vec_map = {} \n",
    "opa2vec_map = {}\n",
    "#with open ('../data/9606.onto2vec_vecs','r') as f:\n",
    "#       for line in f:\n",
    "#           protein, vector=line.strip().split(\" \",maxsplit=1)\n",
    "#           onto2vec_map [protein]=vector\n",
    "with open ('../data/9606.opa2vec_vecs','r') as f:\n",
    "       for line in f:\n",
    "            protein, vector=line.strip().split(\" \",maxsplit=1)\n",
    "            opa2vec_map [protein]=vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pair features for the training/validation/testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "data_type = ['train', 'valid', 'test']\n",
    "for i in data_type:\n",
    "        pair_data = []\n",
    "        feature_vecs =[]\n",
    "        label_map ={}\n",
    "        with open ('../data/{0}/9606.protein.links.v11.0.txt'.format(i),'r') as f1:\n",
    "              for line in f1:\n",
    "                  prot1, prot2 = line.strip().split()\n",
    "                  pair_data.append((prot1,prot2))\n",
    "                  label_map[(prot1, prot2)] = 1\n",
    "        with open ('../data/{0}/9606.negative_interactions.txt'.format(i),'r') as f2:\n",
    "             for line in f2:\n",
    "                  prot1, prot2 = line.strip().split()\n",
    "                  pair_data.append((prot1, prot2))\n",
    "                  label_map[(prot1, prot2)] = 0 \n",
    "        random.shuffle(pair_data)\n",
    "        with open ('../data/{0}/9606.onto2vec_features'.format(i),'w') as f3:\n",
    "                   with open ('../data/{0}/9606.labels'.format(i),'w') as f5:\n",
    "                        with open ('../data/{0}/9606.pairs'.format(i),'w') as f6:\n",
    "                             for prot1, prot2 in pair_data:\n",
    "                                 if (prot1 in opa2vec_map and prot2 in opa2vec_map):\n",
    "                                   f6.write ('{0} {1}\\n'.format(prot1,prot2))\n",
    "                                   f5.write ('{0}\\n'.format(label_map[(prot1,prot2)]))\n",
    "                                   f3.write ('{0} {1}\\n'.format(opa2vec_map[prot1],opa2vec_map[prot2]))   \n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculating cosine similarity to explore neighbors of each protein and finding most similar protein vectors. The interaction prediction is then performed based on similarity value based on the assumption that proteins with highly similar feature vectors are more like to interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine \n",
    "from itertools import islice\n",
    "\n",
    "sim_matrix = {}\n",
    "for prot1, prot2 in pair_data:\n",
    "    #v1_onto = onto2vec_map[prot1]\n",
    "    #v2_onto = onto2vec_map [prot2]\n",
    "    if prot1 in opa2vec_map and prot2 in opa2vec_map: \n",
    "        if not prot1 in sim_matrix:\n",
    "            sim_matrix[prot1] = {}\n",
    "        v1_opa = opa2vec_map[prot1]\n",
    "        v2_opa = opa2vec_map[prot2]\n",
    "\n",
    "        v1_opa = numpy.fromstring(v1_opa, dtype=float, count=-1, sep=' ')\n",
    "        v2_opa = numpy.fromstring(v2_opa, dtype=float, count=-1, sep=' ')\n",
    "\n",
    "        sim_matrix[prot1][prot2] = cosine(v1_opa, v2_opa)\n",
    "        #with open ('../data/{0}/9606.onto_sim'.format(i),'a') as onto_cos:\n",
    "        #    with open ('../data/{0}/9606.opa_sim'.format(i),'a') as opa_cos:\n",
    "        #        #onto_cos.write ('{0}\\n'.format(cosine_onto))\n",
    "        #        opa_cos.write ('{0}\\n'.format(cosine_opa))        \n",
    "        \n",
    "#query =str(sys.argv[1])\n",
    "#n = int (sys.argv[2])\n",
    "#query =\"A0A024RBG1\"\n",
    "#n=10\n",
    "#vectors=numpy.loadtxt(\"../data/9606.opa2vec_vecs\");\n",
    "#text_file=\"../data/train/protein_list\"\n",
    "#classfile=open (text_file)\n",
    "#mylist=[]\n",
    "#for linec in classfile:\n",
    "#    mystr=linec.strip()\n",
    "#    mylist.append(mystr)\n",
    "\n",
    "\n",
    "#3.Mapping Entities to Vectors\n",
    "#vectors_map={}\n",
    "#for i in range(0,len(mylist)):\n",
    "#    vectors_map[mylist[i]]=vectors[i,:]\n",
    "\n",
    "\n",
    "#cosine_sim={}\n",
    "#for x in range(0,len(mylist)):\n",
    "#    if (mylist[x]!=query): \t\n",
    "#        v1=vectors_map[mylist[x]]\n",
    "#        v2=vectors_map[query]\n",
    "#        value=cosine(v1,v2)\n",
    "#        cosine_sim[mylist[x]]=value\n",
    "#classes = mylist\n",
    "##5.Retrieving neighbors \n",
    "#sortedmap=sorted(cosine_sim,key=cosine_sim.get, reverse=True)\n",
    "#iterator=islice(sortedmap,n)\n",
    "#i =1\n",
    "#for d in iterator:\n",
    "#    print (str(i)+\". \"+ str(d) +\"\\t\"+str(cosine_sim[d])+\"\\n\")\n",
    "#    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://9606.ENSP00000349016\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prot_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-bc69c14ee9d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprot_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprot_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# so c, d are protein names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopa2vec_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopa2vec_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopa2vec_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopa2vec_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prot_dict' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def load_test_data(data_file):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = 'http://{0}'.format(it[0])\n",
    "            id2 = 'http://{0}'.format(it[1])\n",
    "            data.append((id1, id2))\n",
    "    return data\n",
    "\n",
    "def compute_rank_roc(ranks, n_prots):\n",
    "    auc_x = list(ranks.keys())\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n_prots)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x) / n_prots\n",
    "    return auc\n",
    "\n",
    "\n",
    "# Load test data and compute ranks for each protein\n",
    "test_data = load_test_data('../data/test/9606.protein.links.v11.0.txt')\n",
    "top1 = 0\n",
    "top10 = 0\n",
    "top100 = 0\n",
    "mean_rank = 0\n",
    "ftop1 = 0\n",
    "ftop10 = 0\n",
    "ftop100 = 0\n",
    "fmean_rank = 0\n",
    "labels = {}\n",
    "preds = {}\n",
    "ranks = {}\n",
    "franks = {}\n",
    "eval_data = test_data\n",
    "n = len(eval_data)\n",
    "for c, d in eval_data:\n",
    "    print(c)\n",
    "    c, d = prot_dict[classes[c]], prot_dict[classes[d]] # so c, d are protein names\n",
    "    labels = np.zeros((len(opa2vec_map), len(opa2vec_map)), dtype=np.int32)\n",
    "    preds = np.zeros((len(opa2vec_map), len(opa2vec_map)), dtype=np.float32)\n",
    "    labels[c, d] = 1\n",
    "    ec = onto2vec_map[c, :]\n",
    "    #er = rembeds[r, :]\n",
    "    #ec += er\n",
    "\n",
    "    # Compute distance\n",
    "    #dst = np.linalg.norm(prot_embeds - ec.reshape(1, -1), axis=1)\n",
    "    #res = numpy.loadtxt('onto_cos.write')\n",
    "    \n",
    "    #now we need to look in the vector map for c and d so we can \n",
    "\n",
    "    preds[c, :] = res\n",
    "    index = rankdata(res, method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        top1 += 1\n",
    "    if rank <= 10:\n",
    "        top10 += 1\n",
    "    if rank <= 100:\n",
    "        top100 += 1\n",
    "    mean_rank += rank\n",
    "    if rank not in ranks:\n",
    "        ranks[rank] = 0\n",
    "    ranks[rank] += 1\n",
    "\n",
    "    # Filtered rank\n",
    "    index = rankdata((res * trlabels[c, :]), method='average')\n",
    "    rank = index[d]\n",
    "    if rank == 1:\n",
    "        ftop1 += 1\n",
    "    if rank <= 10:\n",
    "        ftop10 += 1\n",
    "    if rank <= 100:\n",
    "        ftop100 += 1\n",
    "    fmean_rank += rank\n",
    "\n",
    "    if rank not in franks:\n",
    "        franks[rank] = 0\n",
    "    franks[rank] += 1\n",
    "top1 /= n\n",
    "top10 /= n\n",
    "top100 /= n\n",
    "mean_rank /= n\n",
    "ftop1 /= n\n",
    "ftop10 /= n\n",
    "ftop100 /= n\n",
    "fmean_rank /= n\n",
    "\n",
    "rank_auc = compute_rank_roc(ranks, len(proteins))\n",
    "frank_auc = compute_rank_roc(franks, len(proteins))\n",
    "\n",
    "print('{0} {1} {2} {3}').format(top10, top100, mean_rank, rank_auc)\n",
    "print('{0} {1} {2} {3}').format(ftop10, ftop100, fmean_rank, frank_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from random import randint \n",
    "import numpy as np\n",
    "import time\n",
    "import os \n",
    "import sys \n",
    "import numpy\n",
    "import sklearn \n",
    "\n",
    "#Hyperparameters\n",
    "num_epochs = 100\n",
    "num_classes = 2\n",
    "batch_size = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "#Load dataset \n",
    "X_train_1= numpy.loadtxt(\"data/train/{org_id}.embeddings_1\")\n",
    "X_train_2= numpy.loadtxt(\"data/train/{org_id}.embeddings_2\")\n",
    "y_train= numpy.loadtxt(\"data/train/{org_id}.labels\")\n",
    "\n",
    "X_test_1= numpy.loadtxt(\"data/test/{org_id}.embeddings_1\")\n",
    "X_test_2= numpy.loadtxt(\"data/test/{org_id}.embeddings_2\")\n",
    "y_test= numpy.loadtxt(\"data/test/{org_id}.labels\")\n",
    "\n",
    "#transform to torch\n",
    "train_x1= torch.from_numpy(X_train_1).float()\n",
    "train_x2= torch.from_numpy(X_train_2).float()\n",
    "train_x = [train_x1, train_x2]\n",
    "train_label= torch.from_numpy(y_train).long()\n",
    "\n",
    "\n",
    "test_x1 = torch.from_numpy(X_test_1).float()\n",
    "test_x2 = torch.from_numpy(X_test_2).float()\n",
    "test_x=[test_x1, test_x2]\n",
    "test_label= torch.from_numpy(y_test).long()\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_data.append([train_x, train_label])\n",
    "\n",
    "test_data = []\n",
    "test_data.append([test_x,test_label])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Define Network \n",
    "class Net (nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Net, self).__init__()\n",
    "\t\tself.layer1 = nn.Sequential(\n",
    "\t\t\tnn.Linear (200, 600),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.layer2 = nn.Sequential (\n",
    "\t\t\tnn.Linear (600,400),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.layer3 = nn.Sequential(\n",
    "\t\t\tnn.Linear (400, 200),\n",
    "\t\t\tnn.ReLU())\n",
    "\t\tself.drop_out = nn.Dropout()\n",
    "\t\tself.dis = nn.Linear (200,2)\n",
    "\n",
    "\t\t\t\t\n",
    "\tdef forward (self, data):\n",
    "\t\tres = []\n",
    "\t\tfor i in range(2):\n",
    "\t\t\tx = data[i]\n",
    "\t\t\tout = self.layer1(x)\n",
    "\t\t\tout = self.layer2(out)\n",
    "\t\t\tout = self.layer3(out)\n",
    "\t\t\tout = self.drop_out(out)\n",
    "\t\t\t#out = out.reshape(out.size(0),-1)\n",
    "\t\t\tres.append(out)\n",
    "\t\toutput = torch.abs(res[1] - res[0])\n",
    "\t\t#output = torch.mm(res[1] , res[0])\t\t\n",
    "\t\toutput = self.dis(output)\n",
    "\t\treturn output\n",
    "\n",
    "#Create network \n",
    "network = Net()\n",
    "\n",
    "# Use Cross Entropy for back propagation \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam (network.parameters(),lr=learning_rate)\n",
    "\n",
    "# Train the model \n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range (num_epochs):\n",
    "\tfor i, (train_x, train_label) in enumerate (train_loader):\n",
    "\t\t# Get data\n",
    "\t\tinputs = train_x\n",
    "\t\tlabels = train_label\n",
    "\n",
    "\t\t# Run the forward pass\n",
    "\t\toutputs = network (inputs)\n",
    "\t\toutputs=outputs.reshape(-1,2)\n",
    "\t\tlabels=labels.reshape(-1)\t\t\t\t\n",
    "\t\t#print (outputs.size())\n",
    "\t\t#print (labels.size())\n",
    "\t\tloss = criterion (outputs, labels)\n",
    "\t\tloss_list.append(loss.item())\n",
    "\t\n",
    "\t\t# Back propagation and optimization\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# Get prediction\n",
    "\t\ttotal = labels.size(0)\n",
    "\t\t_,predicted = torch.max(outputs.data,1)\n",
    "\t\tcorrect = (predicted == labels).sum().item()\n",
    "\t\tacc_list.append (correct/total)\n",
    "\t\t\n",
    "\t\t#if (i + 1) % 100 == 0:\n",
    "\t\tprint ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), (correct/total)*100))\n",
    "\n",
    "\n",
    "# Test the model \n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\tfor test_x,test_label in  test_loader:\n",
    "\t\toutputs = network (test_x)\n",
    "\t\tlabels = test_label\n",
    "\t\toutputs=outputs.reshape(-1,2)\t\t\n",
    "\t\tarray = outputs.data.cpu().numpy()\n",
    "\t\tnumpy.savetxt('output.csv',array)\n",
    "\t\tlabels=labels.reshape(-1)\t\n",
    "\t\t_, predicted = torch.max(outputs.data,1)\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t#print ('Accuracy of model on test dataset is: {} %'.format((correct / total) *100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
